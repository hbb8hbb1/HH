#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶å¹¶åŒæ­¥CSVä¸­çš„å‘å¸ƒæ—¶é—´åˆ°æ•°æ®åº“

ä½¿ç”¨æ–¹æ³•:
    python3 process_batch.py --html-dir /path/to/html [--csv /path/to/datas.csv]
    
åŠŸèƒ½:
    1. è§£æHTMLæ–‡ä»¶
    2. AIæ¸…æ´—ä¸ºç»“æ„åŒ–JSON
    3. è‡ªåŠ¨å¯¼å…¥åˆ°MongoDBï¼ˆæ”¯æŒå‘å¸ƒæ—¶é—´åŒæ­¥ï¼‰
"""

import sys
import os
import argparse
from pathlib import Path
from typing import Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

# å¯¼å…¥pipelineçš„æ ¸å¿ƒåŠŸèƒ½
from pipeline import (
    parse_html, 
    process_with_ai, 
    check_ai_api,
    AI_TYPE,
    CONCURRENCY
)

# å¯¼å…¥å·¥å…·æ¨¡å—
from mongodb_utils import import_to_mongodb
from csv_utils import load_csv_times, parse_publish_time


def process_batch(html_dir: Path, csv_path: Optional[Path] = None):
    """æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶"""
    
    # 1. æ£€æŸ¥AI API
    ai_available, ai_msg = check_ai_api()
    if not ai_available:
        print(f"âŒ {ai_msg}")
        return
    print(f"âœ… {ai_msg} (ä½¿ç”¨ {AI_TYPE.upper()} API)\n")
    
    # 2. åŠ è½½CSVæ—¶é—´ä¿¡æ¯ï¼ˆå¦‚æœæä¾›ï¼‰
    times_map = {}
    if csv_path and csv_path.exists():
        times_map = load_csv_times(csv_path)
        print()
    
    # 3. æŸ¥æ‰¾HTMLæ–‡ä»¶
    html_files = sorted(html_dir.glob("*.html"))
    if not html_files:
        print(f"âŒ æœªæ‰¾åˆ°HTMLæ–‡ä»¶: {html_dir}")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
    print(f"âš¡ ä½¿ç”¨å¹¶å‘æ•°: {CONCURRENCY} (å¯é€šè¿‡ç¯å¢ƒå˜é‡ CONCURRENCY è°ƒæ•´)\n")
    print(f"{'='*60}\n")
    
    # 4. å¹¶å‘å¤„ç†æ–‡ä»¶
    stats = {"total": len(html_files), "ok": 0, "failed": 0, "skipped": 0}
    stats_lock = Lock()
    
    def process_single_file(html_path: Path, index: int):
        """å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆç”¨äºå¹¶å‘ï¼‰"""
        file_id = html_path.stem
        publish_time_str = times_map.get(file_id)
        publish_time = parse_publish_time(publish_time_str) if publish_time_str else None
        
        try:
            # è§£æHTML
            raw_data = parse_html(html_path)
            if not raw_data.get("title") or not raw_data.get("originalContentText"):
                raise ValueError("è§£æå¤±è´¥ï¼šç¼ºå°‘titleæˆ–content")
            
            # AIæ¸…æ´—
            final_data = process_with_ai(raw_data)
            
            # å¯¼å…¥åˆ°MongoDBï¼ˆå¸¦æ—¶é—´ï¼‰
            success = import_to_mongodb(final_data, publish_time=publish_time, verbose=False)
            
            if success:
                with stats_lock:
                    stats["ok"] += 1
                time_info = f" (å‘å¸ƒæ—¶é—´: {publish_time_str})" if publish_time_str else ""
                return ("ok", f"âœ… [{index}/{stats['total']}] {html_path.name}{time_info}", None)
            else:
                with stats_lock:
                    stats["failed"] += 1
                return ("failed", f"âŒ [{index}/{stats['total']}] {html_path.name} - å¯¼å…¥å¤±è´¥", None)
                
        except Exception as e:
            with stats_lock:
                stats["failed"] += 1
            return ("failed", f"âŒ [{index}/{stats['total']}] {html_path.name} - {str(e)[:80]}", None)
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=CONCURRENCY) as executor:
        futures = {executor.submit(process_single_file, html_path, i+1): (html_path, i+1) 
                   for i, html_path in enumerate(html_files)}
        
        completed = 0
        for future in as_completed(futures):
            completed += 1
            html_path, index = futures[future]
            try:
                result_type, result_msg, _ = future.result()
                print(result_msg)
                
                # æ¯10ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if completed % 10 == 0:
                    with stats_lock:
                        print(f"\nğŸ“Š è¿›åº¦: {completed}/{stats['total']} (æˆåŠŸ: {stats['ok']}, å¤±è´¥: {stats['failed']}, è·³è¿‡: {stats['skipped']})\n")
            except Exception as e:
                print(f"âŒ [{index}/{stats['total']}] {html_path.name} - å¤„ç†å¼‚å¸¸: {e}")
    
    # 5. è¾“å‡ºç»Ÿè®¡
    print(f"{'='*60}")
    print(f"ğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡ï¼š")
    print(f"   æ€»è®¡: {stats['total']} ä¸ªæ–‡ä»¶")
    print(f"   âœ… æˆåŠŸ: {stats['ok']} ä¸ª")
    print(f"   âŒ å¤±è´¥: {stats['failed']} ä¸ª")
    print(f"   â­ï¸  è·³è¿‡: {stats['skipped']} ä¸ªï¼ˆå·²å­˜åœ¨ï¼‰")
    print(f"\nâœ… æ‰€æœ‰æ•°æ®å·²å¯¼å…¥æ•°æ®åº“ï¼Œå‰ç«¯å¯ä»¥ç«‹å³ä½¿ç”¨æ ‡ç­¾ç­›é€‰ï¼")
    print(f"{'='*60}\n")


def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å¤„ç†HTMLæ–‡ä»¶å¹¶åŒæ­¥CSVå‘å¸ƒæ—¶é—´")
    parser.add_argument("--html-dir", required=True, help="HTMLæ–‡ä»¶ç›®å½•")
    parser.add_argument("--csv", help="CSVæ–‡ä»¶è·¯å¾„ï¼ˆåŒ…å«å‘å¸ƒæ—¶é—´ï¼Œå¯é€‰ï¼‰")
    
    args = parser.parse_args()
    
    html_dir = Path(args.html_dir)
    if not html_dir.exists():
        print(f"âŒ HTMLç›®å½•ä¸å­˜åœ¨: {html_dir}")
        sys.exit(1)
    
    csv_path = Path(args.csv) if args.csv else None
    
    process_batch(html_dir, csv_path)


if __name__ == "__main__":
    main()

